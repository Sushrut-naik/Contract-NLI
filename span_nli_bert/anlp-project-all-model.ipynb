{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9860908,"sourceType":"datasetVersion","datasetId":6051957},{"sourceId":9860929,"sourceType":"datasetVersion","datasetId":6051975},{"sourceId":9872828,"sourceType":"datasetVersion","datasetId":6060849},{"sourceId":9892550,"sourceType":"datasetVersion","datasetId":6075717},{"sourceId":9981047,"sourceType":"datasetVersion","datasetId":6141724},{"sourceId":9982750,"sourceType":"datasetVersion","datasetId":6142970},{"sourceId":9984675,"sourceType":"datasetVersion","datasetId":6144400},{"sourceId":9985747,"sourceType":"datasetVersion","datasetId":6145233},{"sourceId":9985995,"sourceType":"datasetVersion","datasetId":6145410},{"sourceId":9987875,"sourceType":"datasetVersion","datasetId":6146677}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\nimport logging as log\nlog.basicConfig(level=log.DEBUG)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:03.568226Z","iopub.execute_input":"2024-11-23T04:09:03.568862Z","iopub.status.idle":"2024-11-23T04:09:07.794075Z","shell.execute_reply.started":"2024-11-23T04:09:03.568824Z","shell.execute_reply":"2024-11-23T04:09:07.793392Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import sys\n# sys.path.append('../')\n# from baselines.utils import *\nimport os\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nos.environ['WANDB_ENTITY'] = 'contract-nli-db'\nos.environ['WANDB_PROJECT'] = 'contract-nli'\nos.environ['WANDB_LOG_MODEL'] = 'end'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:09.542930Z","iopub.execute_input":"2024-11-23T04:09:09.543991Z","iopub.status.idle":"2024-11-23T04:09:09.549063Z","shell.execute_reply.started":"2024-11-23T04:09:09.543937Z","shell.execute_reply":"2024-11-23T04:09:09.548335Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:10.761424Z","iopub.execute_input":"2024-11-23T04:09:10.761801Z","iopub.status.idle":"2024-11-23T04:09:10.795077Z","shell.execute_reply.started":"2024-11-23T04:09:10.761770Z","shell.execute_reply":"2024-11-23T04:09:10.794212Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"cfg = {\n    \"model_name\": \"distilbert/distilbert-base-uncased\",\n    \"batch_size\": 32,\n    \"train_path\": \"/kaggle/input/dataset-contract/train.json\",\n    \"test_path\": \"/kaggle/input/dataset-contract/test.json\",\n    \"dev_path\": \"/kaggle/input/dataset-contract/dev.json\",\n    \"max_length\": 512,\n    \"models_save_dir\": \"/kaggle/working/saved_model\",\n    \"results_dir\": \"/kaggle/working/results\",\n    \"dataset_dir\": \"/kaggle/working/dataset_dir\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:12.289421Z","iopub.execute_input":"2024-11-23T04:09:12.289800Z","iopub.status.idle":"2024-11-23T04:09:12.294813Z","shell.execute_reply.started":"2024-11-23T04:09:12.289767Z","shell.execute_reply":"2024-11-23T04:09:12.293572Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# create dir if not exists\nfrom pathlib import Path\nPath(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\nPath(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:13.786653Z","iopub.execute_input":"2024-11-23T04:09:13.786988Z","iopub.status.idle":"2024-11-23T04:09:13.791753Z","shell.execute_reply.started":"2024-11-23T04:09:13.786959Z","shell.execute_reply":"2024-11-23T04:09:13.790817Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])\n\ntokenizer.save_pretrained(cfg['models_save_dir'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:15.315333Z","iopub.execute_input":"2024-11-23T04:09:15.315696Z","iopub.status.idle":"2024-11-23T04:09:16.837949Z","shell.execute_reply.started":"2024-11-23T04:09:15.315665Z","shell.execute_reply":"2024-11-23T04:09:16.837094Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cde5ffed96a4b8f9c2de2a1c3efa823"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f486355ae65745ae8d8b4c7cb3021f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c576a88d204bbcb363d7c1a9908742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36bc499fde8948d8bfa5641d2d8596d9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/saved_model/tokenizer_config.json',\n '/kaggle/working/saved_model/special_tokens_map.json',\n '/kaggle/working/saved_model/vocab.txt',\n '/kaggle/working/saved_model/added_tokens.json',\n '/kaggle/working/saved_model/tokenizer.json')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg['models_save_dir'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:21.071920Z","iopub.execute_input":"2024-11-23T04:09:21.072250Z","iopub.status.idle":"2024-11-23T04:09:21.110062Z","shell.execute_reply.started":"2024-11-23T04:09:21.072220Z","shell.execute_reply":"2024-11-23T04:09:21.109380Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip install icecream\nfrom icecream import ic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:22.668289Z","iopub.execute_input":"2024-11-23T04:09:22.668727Z","iopub.status.idle":"2024-11-23T04:09:31.751824Z","shell.execute_reply.started":"2024-11-23T04:09:22.668694Z","shell.execute_reply":"2024-11-23T04:09:31.750826Z"}},"outputs":[{"name":"stdout","text":"Collecting icecream\n  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream) (0.4.6)\nRequirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.18.0)\nRequirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.0.1)\nRequirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.4.1)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\nDownloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\nInstalling collected packages: icecream\nSuccessfully installed icecream-2.1.3\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def get_hypothesis_idx(hypothesis_name):\n    return int(hypothesis_name.split('-')[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:34.289567Z","iopub.execute_input":"2024-11-23T04:09:34.289925Z","iopub.status.idle":"2024-11-23T04:09:34.294798Z","shell.execute_reply.started":"2024-11-23T04:09:34.289893Z","shell.execute_reply":"2024-11-23T04:09:34.293857Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport random\nimport torch\n\nclass NLIDataset(Dataset):\n    def __init__(self, documents, tokenizer, hypothesis, context_sizes, surround_character_size):\n        label_dict = get_labels()\n        self.tokenizer = tokenizer\n\n        self.tokenizer.add_special_tokens({'additional_special_tokens': ['[SPAN]']})\n\n        data_points = []\n        contexts = [{}]\n\n        for context_size in context_sizes:\n            for i, doc in enumerate(documents):\n                char_idx = 0\n                while char_idx < len(doc['text']):\n                    ic(char_idx)\n                    document_spans = doc['spans']\n                    cur_context = {\n                        'doc_id': i,\n                        'start_char_idx': char_idx,\n                        'end_char_idx': char_idx + context_size,\n                        'spans' : [],\n                    }\n\n                    for j, (start, end) in enumerate(document_spans):\n                        if end <= char_idx:\n                            continue\n\n                        cur_context['spans'].append({\n                            'start_char_idx': max(start, char_idx),\n                            'end_char_idx': min(end, char_idx + context_size),\n                            'marked': start >= char_idx and end <= char_idx + context_size,\n                            'span_id': j\n                        })\n\n                        if end > char_idx + context_size:\n                            break\n\n                    if cur_context == contexts[-1]:\n                        char_idx = cur_context['end_char_idx'] - surround_character_size\n                        continue\n\n                    contexts.append(cur_context)\n                    if len(cur_context['spans']) == 1 and not cur_context['spans'][0]['marked']:\n                        char_idx = cur_context['end_char_idx'] - surround_character_size\n                    else:\n                        char_idx = cur_context['spans'][-1]['start_char_idx'] - surround_character_size\n\n        contexts.pop(0)\n\n        for nda_name, nda_desc in hypothesis.items():\n            for i, context in enumerate(contexts):\n\n                nli_label = label_dict[documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['choice']]\n\n                data_point = {}\n                data_point['hypotheis'] = nda_desc\n                cur_premise = \"\"\n                data_point['marked_beg'] = context['spans'][0]['marked']\n                data_point['marked_end'] = context['spans'][-1]['marked']\n                doc_id = context['doc_id']\n                hypothesis_id = get_hypothesis_idx(nda_name)\n                span_ids = []\n\n                if len(context['spans']) == 1:\n                    data_point['marked_end'] = True\n\n                span_labels = []\n\n                for span in context['spans']:\n                    val = int(span['span_id'] in documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['spans'])\n\n                    val = 2 * val - 1 # making 0 -> -1 and 1 -> 1\n\n                    if span['marked']:\n                        span_labels.append(val)\n                        span_ids.append(span['span_id'])\n\n                    cur_premise += ' [SPAN] '\n                    cur_premise += documents[context['doc_id']]['text'][span['start_char_idx']:span['end_char_idx']]\n\n                data_point['premise'] = cur_premise\n                \n                if nli_label == get_labels()['NotMentioned']:\n                    span_labels = torch.zeros(len(span_labels), dtype=torch.long)\n\n                data_point['nli_label'] = torch.tensor(nli_label, dtype=torch.long)\n                data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n                data_point['doc_id'] = torch.tensor(doc_id, dtype=torch.long)\n                data_point['hypothesis_id'] = torch.tensor(hypothesis_id, dtype=torch.long)\n                data_point['span_ids'] = torch.tensor(span_ids, dtype=torch.long)\n\n                data_points.append(data_point)\n\n        self.data_points = data_points\n        self.span_token_id = self.tokenizer.convert_tokens_to_ids('[SPAN]')\n\n    def __len__(self):\n        return len(self.data_points)\n\n    def __getitem__(self, idx):\n        tokenized_data = self.tokenizer(\n            [self.data_points[idx]['hypotheis']],\n            [self.data_points[idx]['premise']],\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n        )\n\n        tokenized_data['input_ids'] = tokenized_data['input_ids'].squeeze()\n        tokenized_data['attention_mask'] = tokenized_data['attention_mask'].squeeze()\n        # tokenized_data['token_type_ids'] = tokenized_data['token_type_ids'].squeeze()\n\n        span_indices = torch.where(tokenized_data['input_ids'] == self.span_token_id)[0]\n\n        if not self.data_points[idx]['marked_beg']:\n            span_indices = span_indices[1:]\n        \n        if not self.data_points[idx]['marked_end'] or tokenized_data['attention_mask'][-1] == 0:\n            span_indices = span_indices[:-1]\n        \n        span_ids = self.data_points[idx]['span_ids']\n        span_ids = span_ids[:len(span_indices)]\n\n        return {\n            'input_ids': tokenized_data['input_ids'],\n            'attention_mask': tokenized_data['attention_mask'],\n            # 'token_type_ids': tokenized_data['token_type_ids'],\n            'span_indices': span_indices,\n            'nli_label': self.data_points[idx]['nli_label'],\n            'span_labels': self.data_points[idx]['span_labels'][:len(span_indices)],\n            'data_for_metrics': {\n                'doc_id': self.data_points[idx]['doc_id'],\n                'hypothesis_id': self.data_points[idx]['hypothesis_id'],\n                'span_ids': span_ids,\n            }\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:35.063977Z","iopub.execute_input":"2024-11-23T04:09:35.064304Z","iopub.status.idle":"2024-11-23T04:09:35.080850Z","shell.execute_reply.started":"2024-11-23T04:09:35.064274Z","shell.execute_reply":"2024-11-23T04:09:35.080033Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\nimport json\nfrom nltk import word_tokenize \nimport re\n\ndef load_data(path: str) -> json:\n    with open(path, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef get_labels() -> dict:\n    return {\n        'NotMentioned': 0,\n        'Entailment': 1,\n        'Contradiction': 2,\n    }\n\ndef get_hypothesis(data: dict) -> list:\n    hypothesis = {}\n    for key, value in data['labels'].items():\n        hypothesis[key] = clean_str(value['hypothesis'])\n    return hypothesis\n\ndef tokenize(str: str) -> str:\n    return ' '.join(word_tokenize(str))\n\ndef clean_str(str: str) -> str:\n    # remove '\\n' character\n    str = str.replace('\\n', ' ')\n    # remove '\\t' character\n    str = re.sub(r'\\\\t', ' ', str)\n    # remove '\\r' character\n    str = re.sub(r'\\\\r', ' ', str)\n    # remove more than 2 consecutive occcurance of a character\n    str = re.sub(r'(.)\\1{2,}', r'\\1', str)\n    return str.strip().lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:35.444454Z","iopub.execute_input":"2024-11-23T04:09:35.444804Z","iopub.status.idle":"2024-11-23T04:09:36.505857Z","shell.execute_reply.started":"2024-11-23T04:09:35.444772Z","shell.execute_reply":"2024-11-23T04:09:36.505161Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_data = load_data(os.path.join(cfg['train_path']))\ndev_data = load_data(os.path.join(cfg['dev_path']))\ntest_data = load_data(os.path.join(cfg['test_path']))\n\nhypothesis = get_hypothesis(train_data)\n\ntrain_data = train_data['documents']\ndev_data = dev_data['documents']\ntest_data = test_data['documents']\n\n\ntrain_data = train_data[:20]\ndev_data = dev_data[:20]\ntest_data = test_data[:20]\n\nic.disable()\n\nic(len(train_data), len(dev_data), len(test_data))\ntrain_dataset = NLIDataset(train_data, tokenizer, hypothesis, [1100], 50)\ndev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1100], 50)\ntest_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1100], 50)\n\nic.enable()\n\ndel train_data\ndel dev_data\ndel test_data\ndel hypothesis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:36.507167Z","iopub.execute_input":"2024-11-23T04:09:36.507573Z","iopub.status.idle":"2024-11-23T04:09:37.465812Z","shell.execute_reply.started":"2024-11-23T04:09:36.507545Z","shell.execute_reply":"2024-11-23T04:09:37.465075Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2882102435.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"ic(len(train_dataset), len(dev_dataset), len(test_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:37.467412Z","iopub.execute_input":"2024-11-23T04:09:37.467784Z","iopub.status.idle":"2024-11-23T04:09:37.681611Z","shell.execute_reply.started":"2024-11-23T04:09:37.467746Z","shell.execute_reply":"2024-11-23T04:09:37.680820Z"}},"outputs":[{"name":"stderr","text":"ic| len(train_dataset): 4080\n    len(dev_dataset): 5236\n    len(test_dataset): 4250\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(4080, 5236, 4250)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\ndef get_class_weights(dataset):\n    nli_labels = [x['nli_label'] for x in dataset]\n\n    span_labels = []\n    for x in dataset:\n        span_labels.extend(x['span_labels'].tolist())\n\n    nli_weights = compute_class_weight('balanced', classes=np.unique(nli_labels), y=np.array(nli_labels))\n\n    nli_weights = nli_weights.tolist()\n\n    span_labels = [x for x in span_labels if x != -1]\n    span_labels = np.array(span_labels)\n    span_weight = np.sum(span_labels == 0) / np.sum(span_labels)\n\n    return nli_weights, span_weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:44.950499Z","iopub.execute_input":"2024-11-23T04:09:44.950859Z","iopub.status.idle":"2024-11-23T04:09:44.957204Z","shell.execute_reply.started":"2024-11-23T04:09:44.950829Z","shell.execute_reply":"2024-11-23T04:09:44.956350Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"nli_weights, span_weight = get_class_weights(train_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:46.881777Z","iopub.execute_input":"2024-11-23T04:09:46.882114Z","iopub.status.idle":"2024-11-23T04:09:56.387912Z","shell.execute_reply.started":"2024-11-23T04:09:46.882082Z","shell.execute_reply":"2024-11-23T04:09:56.387199Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"ic(nli_weights, span_weight)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:56.389124Z","iopub.execute_input":"2024-11-23T04:09:56.389394Z","iopub.status.idle":"2024-11-23T04:09:56.428594Z","shell.execute_reply.started":"2024-11-23T04:09:56.389368Z","shell.execute_reply":"2024-11-23T04:09:56.427794Z"}},"outputs":[{"name":"stderr","text":"ic| nli_weights: [0.8871493803000652, 0.6776283009466866, 2.5185185185185186]\n    span_weight: 28.243243243243242\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"([0.8871493803000652, 0.6776283009466866, 2.5185185185185186],\n 28.243243243243242)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from transformers import PreTrainedModel, PretrainedConfig\n\nclass ContractNLIConfig(PretrainedConfig):\n    # def __init__(self, lambda_ = 1, bert_model_name = cfg['model_name'], num_labels = len(get_labels()), ignore_span_label = 2, nli_weights = nli_weights, span_weight = span_weight, **kwargs):\n    def __init__(self, nli_weights = [1, 1, 1], span_weight = 1, lambda_ = 1, bert_model_name = cfg['model_name'], num_labels = len(get_labels()), ignore_span_label = 2, **kwargs):\n        super().__init__(**kwargs)\n        self.bert_model_name = bert_model_name\n        self.num_labels = num_labels\n        self.lambda_ = lambda_\n        self.ignore_span_label = ignore_span_label\n        self.nli_weights = nli_weights\n        self.span_weight = span_weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:09:58.480794Z","iopub.execute_input":"2024-11-23T04:09:58.481133Z","iopub.status.idle":"2024-11-23T04:09:58.976948Z","shell.execute_reply.started":"2024-11-23T04:09:58.481101Z","shell.execute_reply":"2024-11-23T04:09:58.976239Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoModel\nfrom torch import nn\n\nclass ContractNLI(PreTrainedModel):\n    config_class = ContractNLIConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = AutoModel.from_pretrained(config.bert_model_name)\n        self.bert.resize_token_embeddings(self.bert.config.vocab_size + 1, pad_to_multiple_of=8)\n        # self.bert.eval()\n        # for param in self.bert.parameters():\n        #     param.requires_grad = False\n\n        self.embedding_dim = self.bert.config.hidden_size\n        self.num_labels = config.num_labels\n        self.lambda_ = config.lambda_\n        self.nli_criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.config.nli_weights, dtype=torch.float32))\n        self.span_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.config.span_weight, dtype=torch.float32))\n\n        self.span_classifier = nn.Sequential(\n            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 2, 1)\n        )\n\n        self.nli_classifier = nn.Sequential(\n            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 2, self.num_labels)\n        )\n\n        # initialize weights\n        self.init_weights()\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # use the same initialization as bert\n            module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, input_ids, attention_mask, span_indices):\n        outputs = self.bert(input_ids, attention_mask, output_hidden_states=True).hidden_states[-4:]\n        outputs = torch.stack(outputs, dim=0)\n        outputs = outputs.permute([1, 2, 0, 3])\n        outputs = outputs.reshape([outputs.shape[0], outputs.shape[1], -1])\n\n        gather = torch.gather(outputs, 1, span_indices.unsqueeze(2).expand(-1, -1, outputs.shape[-1]))\n\n        masked_gather = gather[span_indices != 0]\n        span_logits = self.span_classifier(masked_gather)\n        nli_logits = self.nli_classifier(outputs[:, 0, :])\n\n        return span_logits, nli_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:22.866864Z","iopub.execute_input":"2024-11-23T04:10:22.867611Z","iopub.status.idle":"2024-11-23T04:10:22.878456Z","shell.execute_reply.started":"2024-11-23T04:10:22.867574Z","shell.execute_reply":"2024-11-23T04:10:22.877579Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import Trainer\nimport torch\nimport torch.nn.functional as F\n\nclass ContractNLITrainer(Trainer):\n    def __init__(self, *args, data_collator=None, **kwargs):\n        super().__init__(*args, data_collator=data_collator, **kwargs)\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        span_label = inputs.pop('span_labels')\n        nli_label = inputs.pop('nli_label')\n        inputs.pop('data_for_metrics')\n\n        outputs = model(**inputs)\n        span_logits, nli_logits = outputs[0], outputs[1]\n        \n        # span labels = -1, means ignore \n        \n        mask = span_label != -1\n        span_label = span_label[mask]\n        span_logits = span_logits[mask]\n        \n        span_label = span_label.float()\n        span_logits = span_logits.float()\n        \n        span_label = span_label.view(-1)\n        span_logits = span_logits.view(-1)        \n\n        # if len(true_span_labels) == 0 or len(pred_span_labels) != len(true_span_labels):\n        #     span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n        # else:\n        #     span_loss = self.model.span_criterion(pred_span_labels, true_span_labels)\n        \n        if len(span_label) == 0:\n            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n        else:\n            span_loss = self.model.span_criterion(span_logits, span_label)\n\n        nli_loss = self.model.nli_criterion(nli_logits, nli_label)\n\n        if torch.isnan(nli_loss):\n            nli_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n\n        if torch.isnan(span_loss):\n            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n\n        loss = span_loss + self.model.lambda_ * nli_loss\n\n        if loss.item() == 0:\n            loss = torch.tensor(0, dtype=torch.float32, device=DEVICE, requires_grad=True)\n\n        return (loss, outputs) if return_outputs else loss\n\n    @staticmethod\n    def collate_fn(features):\n        # Get the span indices and pad them to the maximum length in the batch\n        span_indices_list = [feature['span_indices'] for feature in features]\n        max_len = max([len(span_indices) for span_indices in span_indices_list])\n        span_indices_list = [torch.cat([span_indices, torch.zeros(max_len - len(span_indices), dtype=torch.long)]) for span_indices in span_indices_list]\n    \n        # Pad span_ids to the maximum length\n        span_ids_list = [feature['data_for_metrics']['span_ids'] for feature in features]\n        max_len_span_ids = max([len(span_ids) for span_ids in span_ids_list])\n        span_ids_list = [torch.cat([span_ids, torch.full((max_len_span_ids - len(span_ids),), -1)]) for span_ids in span_ids_list]\n    \n        # Pad input_ids, attention_mask, and token_type_ids\n        input_ids = [feature['input_ids'] for feature in features]\n        attention_mask = [feature['attention_mask'] for feature in features]\n        # token_type_ids = [feature['token_type_ids'] for feature in features]\n        \n        max_len_input = max([len(ids) for ids in input_ids])\n        input_ids = [F.pad(ids, (0, max_len_input - len(ids))) for ids in input_ids]\n        attention_mask = [F.pad(mask, (0, max_len_input - len(mask))) for mask in attention_mask]\n        # token_type_ids = [F.pad(type_ids, (0, max_len_input - len(type_ids))) for type_ids in token_type_ids]\n    \n        # Stack all padded tensors\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        # token_type_ids = torch.stack(token_type_ids)\n        span_indices = torch.stack(span_indices_list)\n        nli_label = torch.stack([feature['nli_label'] for feature in features])\n        span_label = torch.cat([feature['span_labels'] for feature in features], dim=0)\n        \n        # Prepare the data for metrics\n        data_for_metrics = {\n            'doc_id': torch.stack([feature['data_for_metrics']['doc_id'] for feature in features]),\n            'hypothesis_id': torch.stack([feature['data_for_metrics']['hypothesis_id'] for feature in features]),\n            'span_ids': torch.stack(span_ids_list),\n        }\n    \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            # 'token_type_ids': token_type_ids,\n            'span_indices': span_indices,\n            'nli_label': nli_label,\n            'span_labels': span_label,\n            'data_for_metrics': data_for_metrics,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:23.567986Z","iopub.execute_input":"2024-11-23T04:10:23.568303Z","iopub.status.idle":"2024-11-23T04:10:35.701828Z","shell.execute_reply.started":"2024-11-23T04:10:23.568273Z","shell.execute_reply":"2024-11-23T04:10:35.700886Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom transformers import EarlyStoppingCallback\n\ntraining_args = TrainingArguments(\n    auto_find_batch_size=True,\n    output_dir=cfg['results_dir'],   # output directory\n    num_train_epochs=10,            # total number of training epochs\n    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n    logging_strategy='epoch',\n    eval_steps=1,\n    save_steps=1,\n    logging_steps=1,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    fp16=True,\n    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n    report_to='none',\n    # report_to='wandb',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:35.703279Z","iopub.execute_input":"2024-11-23T04:10:35.703836Z","iopub.status.idle":"2024-11-23T04:10:35.760666Z","shell.execute_reply.started":"2024-11-23T04:10:35.703805Z","shell.execute_reply":"2024-11-23T04:10:35.758206Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def wandb_hp_space(trial):\n    return {\n        \"method\": \"random\",\n        \"metric\": {\n            \"name\": \"eval/loss\",\n            \"goal\": \"minimize\"\n        },\n        \"parameters\": {\n            \"learning_rate\": {\n                \"values\": [1e-5, 3e-5, 5e-5]\n            },\n            \"lambda_\": {\n                \"values\": [0.05, 0.1, 0.4]\n            },\n        }\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:35.761835Z","iopub.execute_input":"2024-11-23T04:10:35.762166Z","iopub.status.idle":"2024-11-23T04:10:35.768018Z","shell.execute_reply.started":"2024-11-23T04:10:35.762130Z","shell.execute_reply":"2024-11-23T04:10:35.767018Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def model_init(trial):\n    if trial is None:\n        return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight))\n\n    return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight, lambda_=trial['lambda_']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:35.769824Z","iopub.execute_input":"2024-11-23T04:10:35.770132Z","iopub.status.idle":"2024-11-23T04:10:35.789641Z","shell.execute_reply.started":"2024-11-23T04:10:35.770097Z","shell.execute_reply":"2024-11-23T04:10:35.788713Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# model = ContractNLI(config).to(DEVICE)\ntrainer = ContractNLITrainer(\n    model=None,                          # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=dev_dataset,            # evaluation dataset\n    data_collator=ContractNLITrainer.collate_fn,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n    model_init=model_init,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:35.790556Z","iopub.execute_input":"2024-11-23T04:10:35.790834Z","iopub.status.idle":"2024-11-23T04:10:38.949911Z","shell.execute_reply.started":"2024-11-23T04:10:35.790810Z","shell.execute_reply":"2024-11-23T04:10:38.948942Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c509dbd5f94e54ab758d28cc5fd6a2"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:18:19.342512Z","iopub.execute_input":"2024-11-22T10:18:19.343123Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6098' max='30480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6098/30480 1:50:49 < 7:23:15, 0.92 it/s, Epoch 2/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.679600</td>\n      <td>3.938724</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='1234' max='1924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1234/1924 02:00 < 01:07, 10.22 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metric\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\nimport logging as log\nlog.basicConfig(level=log.DEBUG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:47.322967Z","iopub.execute_input":"2024-11-23T04:10:47.323855Z","iopub.status.idle":"2024-11-23T04:10:47.327640Z","shell.execute_reply.started":"2024-11-23T04:10:47.323817Z","shell.execute_reply":"2024-11-23T04:10:47.326769Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import sys\nimport os\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nos.environ['WANDB_ENTITY'] = 'contract-nli-db'\nos.environ['WANDB_PROJECT'] = 'contract-nli-metric'\nos.environ['WANDB_LOG_MODEL'] = 'end'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:48.666164Z","iopub.execute_input":"2024-11-23T04:10:48.666843Z","iopub.status.idle":"2024-11-23T04:10:48.671276Z","shell.execute_reply.started":"2024-11-23T04:10:48.666808Z","shell.execute_reply":"2024-11-23T04:10:48.670363Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:50.451993Z","iopub.execute_input":"2024-11-23T04:10:50.452355Z","iopub.status.idle":"2024-11-23T04:10:50.459934Z","shell.execute_reply.started":"2024-11-23T04:10:50.452324Z","shell.execute_reply":"2024-11-23T04:10:50.458729Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import json\n\ncfg = {\n    \"train_path\": \"/kaggle/input/dataset-contract/train.json\",\n    \"test_path\": \"/kaggle/input/dataset-contract/test.json\",\n    \"dev_path\": \"/kaggle/input/dataset-contract/dev.json\",\n    \"model_name\": \"distilbert/distilbert-base-uncased\",\n    \"max_length\": 512,\n    \"models_save_dir\": \"/kaggle/input/anlp-project-trained-model/checkpoint\",\n    \"dataset_dir\": \"./scratch/shu7bh/contract_nli/dataset\",\n    \"results_dir\": \"./scratch/shu7bh/contract_nli/results\",\n    \"trained_model_dir\": \"/kaggle/input/anlp-project-trained-model/\",\n    \"batch_size\": 32\n}\n\ncfg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:52.667827Z","iopub.execute_input":"2024-11-23T04:10:52.668697Z","iopub.status.idle":"2024-11-23T04:10:52.674825Z","shell.execute_reply.started":"2024-11-23T04:10:52.668656Z","shell.execute_reply":"2024-11-23T04:10:52.674003Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'train_path': '/kaggle/input/dataset-contract/train.json',\n 'test_path': '/kaggle/input/dataset-contract/test.json',\n 'dev_path': '/kaggle/input/dataset-contract/dev.json',\n 'model_name': 'distilbert/distilbert-base-uncased',\n 'max_length': 512,\n 'models_save_dir': '/kaggle/input/anlp-project-trained-model/checkpoint',\n 'dataset_dir': './scratch/shu7bh/contract_nli/dataset',\n 'results_dir': './scratch/shu7bh/contract_nli/results',\n 'trained_model_dir': '/kaggle/input/anlp-project-trained-model/',\n 'batch_size': 32}"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# create dir if not exists\nfrom pathlib import Path\nPath(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\nPath(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:53.968439Z","iopub.execute_input":"2024-11-23T04:10:53.968829Z","iopub.status.idle":"2024-11-23T04:10:54.526266Z","shell.execute_reply.started":"2024-11-23T04:10:53.968796Z","shell.execute_reply":"2024-11-23T04:10:54.525069Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/anlp-project-trained-model/checkpoint'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create dir if not exists\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 3\u001b[0m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels_save_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m Path(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:1179\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle/input/anlp-project-trained-model'"],"ename":"OSError","evalue":"[Errno 30] Read-only file system: '/kaggle/input/anlp-project-trained-model'","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:55.358992Z","iopub.execute_input":"2024-11-23T04:10:55.359330Z","iopub.status.idle":"2024-11-23T04:10:55.513237Z","shell.execute_reply.started":"2024-11-23T04:10:55.359300Z","shell.execute_reply":"2024-11-23T04:10:55.512306Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!pip install icecream\nfrom icecream import ic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:10:56.311691Z","iopub.execute_input":"2024-11-23T04:10:56.312421Z","iopub.status.idle":"2024-11-23T04:11:04.234166Z","shell.execute_reply.started":"2024-11-23T04:10:56.312389Z","shell.execute_reply":"2024-11-23T04:11:04.233061Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: icecream in /opt/conda/lib/python3.10/site-packages (2.1.3)\nRequirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream) (0.4.6)\nRequirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.18.0)\nRequirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.0.1)\nRequirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.4.1)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"dev_data = load_data(os.path.join(cfg['dev_path']))\ntest_data = load_data(os.path.join(cfg['test_path']))\n\nhypothesis = get_hypothesis(dev_data)\n\ndev_data = dev_data['documents']\ntest_data = test_data['documents']\n\n# dev_data = dev_data[:50]\n# test_data = test_data[:50]\n\nic.disable()\n\nic(len(dev_data), len(test_data))\ndev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1100], 50)\ntest_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1100], 50)\n\nic.enable()\n\ndel dev_data\ndel test_data\ndel hypothesis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:04.236162Z","iopub.execute_input":"2024-11-23T04:11:04.236470Z","iopub.status.idle":"2024-11-23T04:11:06.443201Z","shell.execute_reply.started":"2024-11-23T04:11:04.236434Z","shell.execute_reply":"2024-11-23T04:11:06.442231Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2882102435.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"print(len(dev_dataset))\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:06.444323Z","iopub.execute_input":"2024-11-23T04:11:06.444646Z","iopub.status.idle":"2024-11-23T04:11:06.448962Z","shell.execute_reply.started":"2024-11-23T04:11:06.444620Z","shell.execute_reply":"2024-11-23T04:11:06.448153Z"}},"outputs":[{"name":"stdout","text":"15385\n28645\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nimport numpy as np\ndef get_micro_average_precision_at_recall(y_true, y_pred, recall_level):\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    return np.interp(recall_level, recall[::-1], precision[::-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:06.450909Z","iopub.execute_input":"2024-11-23T04:11:06.451221Z","iopub.status.idle":"2024-11-23T04:11:06.462201Z","shell.execute_reply.started":"2024-11-23T04:11:06.451188Z","shell.execute_reply":"2024-11-23T04:11:06.461379Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Import numpy and sklearn.metrics\nimport numpy as np\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_score\ndef calculate_micro_average_precision(y_true, y_pred):\n    \"\"\"Calculate the micro average precision score.\n\n    Args:\n        y_true (np.array): True labels.\n        y_pred (np.array): Predicted labels.\n\n    Returns:\n        float: Micro average precision score.\n    \"\"\"\n    # Get the number of classes\n    num_classes = len(np.unique(y_true))\n    \n    if num_classes == 0:\n        return 0.0\n\n    # initialize the average precision score\n    average_precision = 0.0\n\n    # loop over all classes\n    for class_idx in range(num_classes):\n        # get the indices for this class\n        y_true_indices = np.where(y_true == class_idx)\n        # calculate the average precision score for this class\n        average_precision += ic(precision_score(\n            y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n        ))\n\n    # return the average over all classes\n    return average_precision / num_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:06.463122Z","iopub.execute_input":"2024-11-23T04:11:06.463448Z","iopub.status.idle":"2024-11-23T04:11:06.477793Z","shell.execute_reply.started":"2024-11-23T04:11:06.463390Z","shell.execute_reply":"2024-11-23T04:11:06.477025Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef calculate_f1_score_for_class(y_true, y_pred, class_idx):\n    \"\"\"Calculate the F1 score for a given class.\n\n    Args:\n        y_true (np.array): True labels.\n        y_pred (np.array): Predicted labels.\n        class_idx (int): Index of the class.\n\n    Returns:\n        float: F1 score for the given class.\n    \"\"\"\n    # get the indices for the given class\n    y_true_indices = np.where(y_true == class_idx)\n    # calculate the F1 score for the given class\n    return f1_score(\n        y_true[y_true_indices], y_pred[y_true_indices], average=\"macro\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:07.001485Z","iopub.execute_input":"2024-11-23T04:11:07.002212Z","iopub.status.idle":"2024-11-23T04:11:07.006946Z","shell.execute_reply.started":"2024-11-23T04:11:07.002176Z","shell.execute_reply":"2024-11-23T04:11:07.005955Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def precision_at_recall(y_true, y_scores, recall_threshold):\n    precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n    idx = (np.abs(recall - recall_threshold)).argmin()  # Find nearest recall value to threshold\n    ic(threshold[idx])\n    return precision[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:09.175557Z","iopub.execute_input":"2024-11-23T04:11:09.175885Z","iopub.status.idle":"2024-11-23T04:11:09.180499Z","shell.execute_reply.started":"2024-11-23T04:11:09.175860Z","shell.execute_reply":"2024-11-23T04:11:09.179600Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    auto_find_batch_size=True,\n    output_dir=cfg['results_dir'],   # output directory\n    num_train_epochs=10,            # total number of training epochs\n    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n    logging_strategy='epoch',\n    # eval_steps=0.25,\n    # save_steps=0.25,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    # fp16=True,\n    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n    report_to='none',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:10.684250Z","iopub.execute_input":"2024-11-23T04:11:10.684614Z","iopub.status.idle":"2024-11-23T04:11:10.713264Z","shell.execute_reply.started":"2024-11-23T04:11:10.684583Z","shell.execute_reply":"2024-11-23T04:11:10.712392Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"cfg['trained_model_dir']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:13.781467Z","iopub.execute_input":"2024-11-23T04:11:13.782302Z","iopub.status.idle":"2024-11-23T04:11:13.787645Z","shell.execute_reply.started":"2024-11-23T04:11:13.782265Z","shell.execute_reply":"2024-11-23T04:11:13.786835Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/anlp-project-trained-model/'"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# Define the directory where your model is stored locally.\nartifact_dir = '/kaggle/input/full-finetune-bert-base/checkpoint-15242'  # Replace this with the actual path\n\n# Load the model directly from the local directory\nmodel = ContractNLI.from_pretrained(artifact_dir).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:21.538887Z","iopub.execute_input":"2024-11-23T04:11:21.539604Z","iopub.status.idle":"2024-11-23T04:11:26.095818Z","shell.execute_reply.started":"2024-11-23T04:11:21.539568Z","shell.execute_reply":"2024-11-23T04:11:26.095068Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from transformers import Trainer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score\nfrom tqdm import tqdm\nimport numpy as np\n\nclass ContractNLIMetricTrainer(ContractNLITrainer):\n    def __init__(self, *args, data_collator=None, **kwargs):\n        super().__init__(*args, data_collator=data_collator, **kwargs)\n\n    def evaluate(self, eval_dataset=None, ignore_keys=None):\n        self.model.eval()\n        self.dataloader = ic(self.get_eval_dataloader(eval_dataset))\n\n        eval_nli_labels = []\n        eval_nli_preds = []\n        true_labels_per_span = {}\n        probs_per_span = {}\n\n        nli_metrics = {}\n\n        for inputs in tqdm(self.dataloader):\n            inputs = self._prepare_inputs(inputs)\n            span_labels = inputs.pop('span_labels')\n            nli_labels = inputs.pop('nli_label')\n            data_for_metrics = inputs.pop('data_for_metrics')\n\n            span_indices_to_consider = torch.where(span_labels != -1)[0]\n\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n                span_logits, nli_logits = outputs[0], outputs[1]\n\n                span_labels = span_labels.float()\n                span_logits = span_logits.float()\n                \n                span_labels = span_labels.view(-1)\n                span_logits = span_logits.view(-1)\n\n                # start_index = 0\n                \n                indices_considered = 0 # total number of span indices considered\n\n                # find the corresponding span index in data_for_metrics['span_ids'] considering -1 to be padding index\n                # ic(span_index)\n                for i, span_index_row in enumerate(data_for_metrics['span_ids']):\n                    current_index = 0 # current row's first -1 index\n                    # ic(span_index_row)\n                    first_minus_one_index = torch.where(span_index_row == -1)[0]\n                    # ic(first_minus_one_index)\n                    if len(first_minus_one_index) == 0:\n                        first_minus_one_index = len(span_index_row)\n                    else:\n                        first_minus_one_index = first_minus_one_index[0].item()\n\n                    key = str(data_for_metrics['doc_id'][i].item())+ '-' + str(data_for_metrics['hypothesis_id'][i].item())\n\n                    # mask span_labels and span_logits for the current row\n                    mask = span_labels[indices_considered:indices_considered+first_minus_one_index] != -1\n                    span_logits_masked = span_logits[indices_considered:indices_considered+first_minus_one_index][mask]\n\n                    spans_contribution = torch.sum(torch.sigmoid(span_logits_masked)) / (len(span_logits_masked)) \n\n                    if key in nli_metrics:\n                        nli_metrics[key]['spans_contribution'].append(spans_contribution)\n                        nli_metrics[key]['nli_logits'].append(nli_logits[i])\n                    else:\n                        nli_metrics[key] = {}\n                        nli_metrics[key]['true_nli_labels'] = nli_labels[i]\n                        nli_metrics[key]['spans_contribution'] = [spans_contribution]\n                        nli_metrics[key]['nli_logits'] = [nli_logits[i]]\n                    \n                    current_index = first_minus_one_index\n                    indices_considered += current_index\n                    \n                    # ic(indices_considered)\n                    # ic(current_index)\n                    cnt = 0 # count to keep track of the number of span indices added in dictionary\n                    \n                    for span_index in span_indices_to_consider:\n\n                        if span_index < indices_considered:\n                            cnt += 1\n                            value_index = span_index - (indices_considered - current_index)\n                            doc_id = data_for_metrics['doc_id'][i]\n                            hypothesis_id = data_for_metrics['hypothesis_id'][i]\n                            span_id = data_for_metrics['span_ids'][i][value_index]\n                            key = str(doc_id)+ '-' + str(hypothesis_id)+ '-' + str(span_id)\n                            true_labels_per_span[key] = span_labels[span_index]\n                            if key in probs_per_span:\n                                probs_per_span[key].append(torch.sigmoid(span_logits[span_index]))\n                                # probs_per_span[key].append(span_logits[value_index])\n                            else:\n                                probs_per_span[key] = [torch.sigmoid(span_logits[span_index])]\n                                # probs_per_span[key] = [span_logits[value_index]]\n                        else: \n                            break \n                    \n                    span_indices_to_consider = span_indices_to_consider[cnt:]\n\n                # eval_span_preds = torch.tensor(eval_span_preds.squeeze(1), dtype=torch.long)\n\n                nli_preds = torch.argmax(torch.softmax(nli_logits, dim=1), dim=1)\n                eval_nli_labels.extend(nli_labels.cpu().numpy())\n                eval_nli_preds.extend(nli_preds.cpu().numpy())\n        \n        eval_span_labels = []\n        eval_span_preds = []\n        span_ids_for_eval = []\n        \n        for key in true_labels_per_span:\n            eval_span_labels.append(true_labels_per_span[key].item())\n            eval_span_preds.append(torch.mean(torch.stack(probs_per_span[key])).item())\n            span_id = key.split('-')[-1]  # Extract the span ID from the key (last part after '-')\n            span_ids_for_eval.append(span_id)\n\n        # Print the true labels, predicted labels, and span IDs\n\n        eval_nli_acc = accuracy_score(eval_nli_labels, eval_nli_preds)\n\n        ic.enable()\n        # ic(list(zip(eval_span_labels, eval_span_preds, span_ids_for_eval)))\n        \n        # ic(list(zip(eval_span_labels, eval_span_preds)))\n        # ic(len(eval_span_labels), len(eval_span_preds))\n        # ic(sum(eval_span_labels), sum(eval_span_preds))\n\n        # find threshold for 80% recall\n        # precision, recall, thresholds = precision_recall_curve(eval_span_labels, eval_span_preds)\n\n\n        mAP = (average_precision_score(eval_span_labels, eval_span_preds, pos_label=0) + average_precision_score(eval_span_labels, eval_span_preds, pos_label=1))/2\n\n        # mAP = average_precision_score(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))\n        precision_at_80_recall = precision_at_recall(torch.tensor(eval_span_labels), torch.tensor(eval_span_preds), 0.8)\n        f1_score_for_entailment = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Entailment'])\n        f1_score_for_contradiction = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Contradiction'])\n        \n        return {\n            'mAP' : mAP,\n            'precision_at_80_recall' : precision_at_80_recall,\n            'nli_acc': eval_nli_acc,\n            'f1_score_for_entailment': f1_score_for_entailment,\n            'f1_score_for_contradiction': f1_score_for_contradiction\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:28.011709Z","iopub.execute_input":"2024-11-23T04:11:28.012017Z","iopub.status.idle":"2024-11-23T04:11:28.029879Z","shell.execute_reply.started":"2024-11-23T04:11:28.011992Z","shell.execute_reply":"2024-11-23T04:11:28.029021Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"trainer = ContractNLIMetricTrainer(\n    model=model,                          # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    # train_dataset=train_dataset,         # training dataset\n    eval_dataset=dev_dataset,            # evaluation dataset\n    data_collator=ContractNLIMetricTrainer.collate_fn,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:30.881230Z","iopub.execute_input":"2024-11-23T04:11:30.882038Z","iopub.status.idle":"2024-11-23T04:11:30.922280Z","shell.execute_reply.started":"2024-11-23T04:11:30.882001Z","shell.execute_reply":"2024-11-23T04:11:30.921423Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"ic.disable()\n# ic.enable()\nresults = trainer.evaluate()\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:11:35.299854Z","iopub.execute_input":"2024-11-23T04:11:35.300201Z","iopub.status.idle":"2024-11-23T04:14:46.739395Z","shell.execute_reply.started":"2024-11-23T04:11:35.300169Z","shell.execute_reply":"2024-11-23T04:14:46.738635Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1924/1924 [03:09<00:00, 10.17it/s]\nic| threshold[idx]: 0.99910885\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'mAP': 0.8378346383422673,\n 'precision_at_80_recall': 0.6778009742519137,\n 'nli_acc': 0.6693532661683458,\n 'f1_score_for_entailment': 0.2646199888977385,\n 'f1_score_for_contradiction': 0.25508741759816567}"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# single Inference Code\n","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"dev_data = load_data(os.path.join(cfg['dev_path']))\ntest_data = load_data(os.path.join(cfg['test_path']))\n\nhypothesis = get_hypothesis(dev_data)\n\ndev_data = dev_data['documents']\ntest_data = test_data['documents']\n\ndev_data = dev_data[:1]\ntest_data = test_data[:1]\n\nic.disable()\n\nic(len(dev_data), len(test_data))\ndev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1100], 50)\ntest_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1100], 50)\n\nprint(\"---------------------------------------------------\")\nprint ( test_data )\nprint(\"---------------------------------------------------\")\nprint ( test_dataset[0])\nprint(\"---------------------------------------------------\")\n\nic.enable()\n\ndel dev_data\ndel test_data\ndel hypothesis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T16:54:35.046492Z","iopub.execute_input":"2024-11-13T16:54:35.047214Z","iopub.status.idle":"2024-11-13T16:54:35.132403Z","shell.execute_reply.started":"2024-11-13T16:54:35.047174Z","shell.execute_reply":"2024-11-13T16:54:35.131539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(dev_dataset))\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T16:54:35.706198Z","iopub.execute_input":"2024-11-13T16:54:35.706804Z","iopub.status.idle":"2024-11-13T16:54:35.711328Z","shell.execute_reply.started":"2024-11-13T16:54:35.706767Z","shell.execute_reply":"2024-11-13T16:54:35.710493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    auto_find_batch_size=True,\n    output_dir=cfg['results_dir'],   # output directory\n    num_train_epochs=10,            # total number of training epochs\n    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n    logging_strategy='epoch',\n    # eval_steps=0.25,\n    # save_steps=0.25,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    # fp16=True,\n    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n    report_to='none',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T16:54:37.052387Z","iopub.execute_input":"2024-11-13T16:54:37.052968Z","iopub.status.idle":"2024-11-13T16:54:37.081248Z","shell.execute_reply.started":"2024-11-13T16:54:37.052929Z","shell.execute_reply":"2024-11-13T16:54:37.080412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = ContractNLIMetricTrainer(\n    model=model,                          # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    # train_dataset=train_dataset,         # training dataset\n    eval_dataset=dev_dataset,            # evaluation dataset\n    data_collator=ContractNLIMetricTrainer.collate_fn,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T16:54:38.978197Z","iopub.execute_input":"2024-11-13T16:54:38.979033Z","iopub.status.idle":"2024-11-13T16:54:38.995879Z","shell.execute_reply.started":"2024-11-13T16:54:38.978990Z","shell.execute_reply":"2024-11-13T16:54:38.994997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ic.disable()\n# ic.enable()\nresults = trainer.evaluate()\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T16:54:40.332551Z","iopub.execute_input":"2024-11-13T16:54:40.332887Z","iopub.status.idle":"2024-11-13T16:54:47.911538Z","shell.execute_reply.started":"2024-11-13T16:54:40.332855Z","shell.execute_reply":"2024-11-13T16:54:47.910650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = load_data(os.path.join(cfg['test_path']))\ntest_data = test_data['documents']\ntest_data = test_data[:1]\ntest_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}