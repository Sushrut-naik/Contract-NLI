{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9560277,"sourceType":"datasetVersion","datasetId":5825865}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nimport numpy as np\nfrom tqdm import tqdm\nimport re\nimport json\n\n# Configuration dictionary from the utils\ncfg = {\n    'train_path': '/kaggle/input/contract-nli/train.json',\n    'test_path': '/kaggle/input/contract-nli/test.json',\n    'dev_path': '/kaggle/input/contract-nli/dev.json',\n    \"model_name\": \"bert-base-uncased\",\n    \"max_length\": 512,\n    \"models_save_dir\": \"./scratch/shu7bh/contract_nli/models\",\n    \"dataset_dir\": \"./scratch/shu7bh/contract_nli/dataset\",\n    \"results_dir\": \"./scratch/shu7bh/contract_nli/results\",\n}\n\n# Load data function from the utils\ndef load_data(path: str) -> dict:\n    with open(path, 'r') as f:\n        data = json.load(f)\n    return data\n\n# Get labels function from the utils\ndef get_labels() -> dict:\n    return {\n        'NotMentioned': 0,\n        'Entailment': 1,\n        'Contradiction': 2,\n    }\n\n# Get hypothesis function from the utils\ndef get_hypothesis(data: dict) -> dict:\n    hypothesis = {}\n    for key, value in data['labels'].items():\n        hypothesis[key] = clean_str(value['hypothesis'])\n    return hypothesis\n\n# Tokenize and clean string functions from the utils\ndef clean_str(string: str) -> str:\n    string = string.replace('\\n', ' ')\n    string = re.sub(r'\\\\t', ' ', string)\n    string = re.sub(r'\\\\r', ' ', string)\n    string = re.sub(r'(.)\\1{2,}', r'\\1', string)  # Remove more than 2 consecutive occurrences of a character\n    return string.strip().lower()\n\n# Function to clean data\ndef clean_data(data: dict) -> None:\n    for i in range(len(data['documents'])):\n        data['documents'][i]['text'] = clean_str(data['documents'][i]['text'])\n\n# Compute predicted and true labels using cosine similarity\ndef get_Ypred_Ytrue(data: dict, tfidf: TfidfVectorizer, hypothesis: dict) -> (list, list):\n    Y_pred = []\n    Y_true = []\n    hypothesis_vecs = {}\n\n    for key, val in hypothesis.items():\n        hypothesis_vecs[key] = tfidf.transform([val])\n\n    for i in tqdm(range(len(data[\"documents\"]))):\n        doc_text = data[\"documents\"][i][\"text\"]\n        Y_pred_curdoc = []\n        Y_true_curdoc = []\n\n        for key, val in hypothesis.items():\n            choice = data[\"documents\"][i][\"annotation_sets\"][0][\"annotations\"][key][\"choice\"]\n            if choice == \"NotMentioned\":\n                continue\n\n            spans_for_hypothesis = data[\"documents\"][i][\"annotation_sets\"][0][\"annotations\"][key][\"spans\"]\n\n            for j, span in enumerate(data[\"documents\"][i][\"spans\"]):\n                # Check if span is a dictionary, otherwise attempt conversion\n                if isinstance(span, dict):\n                    start, end = span['start'], span['end']\n                else:\n                    # Convert span to a dictionary if it's a list or another structure\n#                     print(f\"Converting span: {span}\")  # Debug: Print to observe the structure\n                    try:\n                        # Assume the span might be a list of two items [start, end] or other simple structures\n                        start, end = span[0], span[1]\n                    except (TypeError, IndexError):\n#                         print(f\"Skipping invalid span format: {span}\")\n                        continue  # Skip this span if it can't be processed\n\n                # Process the span text and compute similarity\n                span_text = doc_text[start:end]\n                span_vec = tfidf.transform([span_text])\n                cosine_sim = cosine_similarity(hypothesis_vecs[key], span_vec)[0][0]\n                \n                # Append predictions and true labels\n                Y_pred_curdoc.append(1 if cosine_sim >= 0.4 else 0)\n\n                # Check if each span in spans_for_hypothesis is a dictionary and extract start, end\n#                 if isinstance(spans_for_hypothesis, list):\n#                     Y_true_curdoc.append(1 if any(\n#                         isinstance(span_hypo, dict) and span_hypo.get(\"start\") == start and span_hypo.get(\"end\") == end\n#                         for span_hypo in spans_for_hypothesis\n#                     ) else 0)\n#                 else:\n#                     print(f\"Invalid format for spans_for_hypothesis: {spans_for_hypothesis}\")\n#                     Y_true_curdoc.append(0)\n                Y_true_curdoc.append(1 if j in spans_for_hypothesis else 0)\n\n        Y_pred.append(Y_pred_curdoc)\n        Y_true.append(Y_true_curdoc)\n\n    return Y_pred, Y_true\n\n# Precision calculation at 80% recall\ndef precision_at_80_recall(ypred, ytrue):\n    precision, recall, thresholds = precision_recall_curve(ytrue, ypred)\n    idx = (abs(recall - 0.8)).argmin()\n    return precision[idx]\n\n# Mean average precision calculation\ndef mean_average_precision(Y_pred, Y_true):\n    average_prec_scores = []\n    for i in range(len(Y_true)):\n        average_prec_scores.append(average_precision_score(Y_true[i], Y_pred[i], average='micro'))\n    return np.mean(average_prec_scores)\n\n# Load your test data and hypothesis\ntest = load_data(cfg['test_path'])\nhypothesis = get_hypothesis(test)\n\n# Create and fit the TF-IDF vectorizer on all documents\nall_text = [doc['text'] for doc in test['documents']]\ntfidf = TfidfVectorizer().fit(all_text)\n\n# Get predicted and true labels\nY_pred_test, Y_true_test = get_Ypred_Ytrue(test, tfidf, hypothesis)\n\n# Calculate precision at 80% recall and mean average precision\nprec_arr = []\nfor i in range(len(Y_true_test)):\n    prec_arr.append(precision_at_80_recall(Y_pred_test[i], Y_true_test[i]))\n\nprint(\"Precision @ 80% recall: \", np.mean(np.array(prec_arr)))\nprint(\"Mean Average Precision: \", mean_average_precision(Y_pred_test, Y_true_test))","metadata":{"_uuid":"4af34441-671a-47ce-85d1-919cc1be67dc","_cell_guid":"e8dd2eba-fcd9-4106-a651-9758015dda9b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T12:18:38.074365Z","iopub.execute_input":"2024-10-07T12:18:38.074745Z","iopub.status.idle":"2024-10-07T12:20:38.807517Z","shell.execute_reply.started":"2024-10-07T12:18:38.074690Z","shell.execute_reply":"2024-10-07T12:20:38.806474Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"100%|██████████| 123/123 [01:58<00:00,  1.03it/s]","output_type":"stream"},{"name":"stdout","text":"Precision @ 80% recall:  0.030058717670690627\nMean Average Precision:  0.0461261085908014\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"_uuid":"28f174f2-c68f-4aa5-86af-1c22e089606a","_cell_guid":"27c98216-6ae6-4405-b038-15513d9feceb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}